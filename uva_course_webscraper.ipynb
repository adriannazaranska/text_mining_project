{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb25471d-d703-45ce-8d36-d36280180d2f",
   "metadata": {},
   "source": [
    "# Scraper for Uva Courses \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42077de3-2d68-4667-a945-541a1ea3cff1",
   "metadata": {},
   "source": [
    "NOTE: AI, specifically ChatGPT and Deepseek, were used to generate almost all of the code present within this notebook. The reason for this is because parsing online websites for data with libraries such as BS4 and Selenium is outside the scope of the course. For this reason, I worked one level of abstraction higher than the code, understanding the inputs and outputs of what the code generated was doing, but not fully understanding the application.\n",
    "\n",
    "The first part of the notebook deals with scraping all the course links from the UvA course catalogue. This code was generated using ChatGPT, and slightly tweaked. Only the first three prompts in the conversation were used to generate the code, the rest of the conversation did not end up being used. The transcript for this conversation can be found here: https://chatgpt.com/share/6834a603-651c-800d-9f41-0290db8d714d \n",
    "\n",
    "The second part of the notebook deals with parsing the HTML of each course link to find information useful for the recommender system. This code was generated using Deepseek. However, Deepseek does not yet provide the option to share conversations, so instead each prompt used will be put in a Markdown box at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce9750a-2fb3-44cc-909d-1f3f9bd36ee7",
   "metadata": {},
   "source": [
    "## Scraping Course Links from Course Catalogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075f639-9e9e-4cea-8be0-485bb9d6b399",
   "metadata": {},
   "source": [
    "First, we import the necessary libraries to create a function to scrape the webpage links of all the courses from the UvA course catalogue website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc802de-ea73-4900-aac8-da902d5d17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2aa14ad-198c-4a06-a4d5-30789fd861f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57b8bd1-914e-4305-bd7f-33ba2e1c39e4",
   "metadata": {},
   "source": [
    "Next, we create the function itself. The UvA course catalogue contains 20 courses per page, so the function needs to parse the html of the current page, find the course links of the 20 courses, and then click on the \"next\" button to go to the next page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "845dd0f5-1293-41a3-8628-e43562a31f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_course_links():\n",
    "    base_url = \"https://studiegids.uva.nl/xmlpages/page/2024-2025-en/search-course\"\n",
    "    course_links = []\n",
    "\n",
    "    # Set up headless Chrome browser\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    driver.get(base_url)\n",
    "\n",
    "    # There's 20 courses per page, and 3802 courses, so 200 max pages = 4000 courses just to be safe. I also decreased\n",
    "    # max_pages when running the code for bug fixing so as not to scrape the entire catalogue each time.\n",
    "    cur_page = 0\n",
    "    max_pages = 200\n",
    "\n",
    "    while cur_page<max_pages:\n",
    "        time.sleep(2)  # wait for JS content to load\n",
    "\n",
    "        # Get the page source again and re-fetch elements\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"search-results\")))\n",
    "\n",
    "        # Get course links freshly on each iteration\n",
    "        course_elements = driver.find_elements(By.CSS_SELECTOR, \"div#search-results a[href*='/search-course/course/']\")\n",
    "        for i in range(len(course_elements)):\n",
    "            try:\n",
    "                # Refresh the element to avoid stale reference\n",
    "                element = driver.find_elements(By.CSS_SELECTOR, \"div#search-results a[href*='/search-course/course/']\")[i]\n",
    "                link = element.get_attribute(\"href\")\n",
    "                if link and link not in course_links:\n",
    "                    course_links.append(link)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping element due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Try clicking \"next\"\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[contains(text(), 'next')]\")))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            time.sleep(1)\n",
    "            next_button.click()\n",
    "            \n",
    "            # Wait for page to change (wait for current button to become stale)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.staleness_of(next_button)\n",
    "            )\n",
    "            \n",
    "            # Wait for new results\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.ID, \"search-results\"))\n",
    "            )\n",
    "            time.sleep(1)  # Additional buffer\n",
    "            \n",
    "            cur_page += 1\n",
    "        except:\n",
    "            print(\"No more pages found. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return course_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ec5c2-0626-481d-a673-a520c48b3bc1",
   "metadata": {},
   "source": [
    "Now, we can simply execute the function and save the course links inside a .txt file, which we can later parse to obtain the information we need for the recommender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb9c437e-8b93-4d07-9c51-92d877dcac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages found. Exiting loop.\n",
      "Found 3812 courses.\n"
     ]
    }
   ],
   "source": [
    "links = scrape_course_links()\n",
    "print(f\"Found {len(links)} courses.\")\n",
    "    \n",
    "output_file = \"datasets/uva_course_links.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for link in links:\n",
    "        if re.search(r\"studiegids\\.uva\\.nl/xmlpages/page/2024-2025-en/search-course/course/\\d{6}\", link):\n",
    "            f.write(link + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242cf7dc-5197-46ca-83af-a9ec5030326b",
   "metadata": {},
   "source": [
    "## Obtaining Dataset from Course Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e057c-0b72-49ac-8907-4afd95cd6a1d",
   "metadata": {},
   "source": [
    "Now, we need to parse the html of the website at each course link in order to obtain the data for the dataset. Each website contains a table in which we find the following datapoints:\n",
    "1. Course name\n",
    "2. Course catalogue number\n",
    "3. College/graduate\n",
    "4. Language of instruction\n",
    "5. Time period\n",
    "6. Is part of\n",
    "\n",
    "Underneath the table there is also a course description, which is the most important piece of information for the recommender, as this is what we are going to use to compute the similarity between courses. The course description typically contains a few headings, but the ones we will be obtaining are 'Objectives' and 'Contents', as these are the most descriptive of the course itself. We don't think that other headings such as \"Study materials\" or \"Assessment\" etc. would be useful, as these sections do not contain many discriminative tokens that would differentiate the course from others, leading to noise.\n",
    "\n",
    "We will utilise beautiful soup 4 to parse the HTML of each course link, and use pandas to process the information into a dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b962caf-15ba-46c0-9c54-d1cd3348ec51",
   "metadata": {},
   "source": [
    "First, we will define a function that takes the cells i.e. the different sections, of the table in each course link as the parameter and return the corresponding time-period. The reason this needs a separate function is because in the HTML of each course link the semester the course is held is represented with boxes, which makes it more difficult to directly scrape the time period the course is available for. This function will be utilised in a function later on in which we will scrape all the information from the course website simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a876dae8-57bf-48c8-9aa3-b84a0fb132f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d68207b8-bf82-4a92-96b2-b178324fb42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_period(cells):\n",
    "    try:\n",
    "        time_periods = []\n",
    "        \n",
    "        # Find the time period table\n",
    "        time_table = cells[1].find('table')\n",
    "        if not time_table:\n",
    "            return None\n",
    "        \n",
    "        # Process each semester column\n",
    "        for sem_col in time_table.find_all('td', class_=lambda x: x and ('sem1' in x or 'sem2' in x)):\n",
    "            # Determine semester and base number\n",
    "            semester = 1 if 'sem1' in sem_col.get('class', []) else 2\n",
    "            base_num = 0 if semester == 1 else 3\n",
    "            \n",
    "            # Find all red blocks that aren't hidden\n",
    "            red_blocks = sem_col.select('div.red:not(.hide)')\n",
    "            \n",
    "            for block in red_blocks:\n",
    "                # Get block number from class (handles block-2 and block-2-3 formats)\n",
    "                block_class = next((c for c in block.get('class', []) if c.startswith('block-')), None)\n",
    "                if not block_class:\n",
    "                    continue\n",
    "                \n",
    "                # Extract numbers\n",
    "                parts = block_class.split('-')[1:]\n",
    "                if not parts:\n",
    "                    continue\n",
    "                \n",
    "                # Convert to final numbers\n",
    "                if len(parts) == 1:  # Single block (e.g., block-2)\n",
    "                    block_num = int(parts[0])\n",
    "                    time_periods.append(str(base_num + block_num))\n",
    "                else:  # Range (e.g., block-2-3)\n",
    "                    start = int(parts[0])\n",
    "                    end = int(parts[1])\n",
    "                    time_periods.append(f\"{base_num + start}-{base_num + end}\")\n",
    "                    \n",
    "        return ', '.join(time_periods) if time_periods else None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing time period: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe4168-1675-4703-926e-62426b64ba7e",
   "metadata": {},
   "source": [
    "Next, we will define a function that takes a course link as the parameter and returns a dictionary containing the table section as the keys, and the information itself as the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "924fea59-22d8-437c-9612-81f7a7ae6b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_course_info(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url.strip(), headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract course name\n",
    "        breadcrumb = soup.find('nav', id='breadcrumb')\n",
    "        course_name = breadcrumb.find('li', class_='current').find('a').get_text(strip=True) if breadcrumb else None\n",
    "        \n",
    "        # Initialize course data (all fields will be strings)\n",
    "        course_data = {\n",
    "            'url': url.strip(),\n",
    "            'course_name': course_name,\n",
    "            'course_catalogue_number': '',\n",
    "            'college_graduate': '',\n",
    "            'language_of_instruction': '',\n",
    "            'time_period': '',\n",
    "            'is_part_of': '',\n",
    "            'course_description': ''\n",
    "        }\n",
    "\n",
    "        # Extract table data\n",
    "        item_info = soup.find('div', class_='item-info')\n",
    "        if item_info:\n",
    "            for row in item_info.find_all('tr'):\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 2:\n",
    "                    label = cells[0].get_text(strip=True).lower()\n",
    "                    value = cells[1].get_text(strip=True)\n",
    "                    if 'course catalogue number' in label:\n",
    "                        course_data['course_catalogue_number'] = value\n",
    "                    elif 'language of instruction' in label:\n",
    "                        course_data['language_of_instruction'] = value\n",
    "                    elif 'time period(s)' in label:\n",
    "                        course_data['time_period'] = extract_time_period(cells)\n",
    "                    elif 'college/graduate' in label:\n",
    "                        course_data['college_graduate'] = value\n",
    "                    elif 'is part of' in label:\n",
    "                        links = [link.get_text(strip=True) for link in cells[1].find_all('a')]\n",
    "                        course_data['is_part_of'] = ', '.join(links) if links else ''\n",
    "\n",
    "        # Extract description sections\n",
    "        def extract_section(heading_id):\n",
    "            heading = soup.find('h4', id=heading_id)\n",
    "            if not heading:\n",
    "                return ''\n",
    "            \n",
    "            content = []\n",
    "            next_elem = heading.find_next_sibling()\n",
    "            while next_elem and next_elem.name != 'h4':\n",
    "                if next_elem.name == 'p':\n",
    "                    content.append(next_elem.get_text(strip=True))\n",
    "                elif next_elem.name == 'ul':\n",
    "                    content.extend(li.get_text(strip=True) for li in next_elem.find_all('li'))\n",
    "                next_elem = next_elem.find_next_sibling()\n",
    "            \n",
    "            if content:\n",
    "                return ' '.join(content)\n",
    "            return ''\n",
    "\n",
    "        objectives = extract_section('leerdoel')\n",
    "        contents = extract_section('inhoud')\n",
    "        \n",
    "        # Combine all description parts\n",
    "        description_parts = [part for part in [objectives, contents] if part]\n",
    "        if description_parts:\n",
    "            course_data['course_description'] = '\\n'.join(description_parts)\n",
    "\n",
    "        return course_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url.strip()}: {str(e)}\")\n",
    "        return {\n",
    "            'url': url.strip(),\n",
    "            'course_name': '',\n",
    "            'course_catalogue_number': '',\n",
    "            'college_graduate': '',\n",
    "            'language_of_instruction': '',\n",
    "            'is_part_of': '',\n",
    "            'course_description': '',\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890fc5a4-195b-4d72-9a93-cb34c7656304",
   "metadata": {},
   "source": [
    "Now we can simply read the course links from the .txt file we created earlier, scrape the information from each course link utilising the functions we just made, save this information in a pandas dataframe, and then convert the dataframe into a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ac3a241-f832-41a4-bf04-0865c134cd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping courses: 100%|█████████████████████████████████████████████████████████████| 3812/3812 [15:01<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 3812 courses to datasets/recommender_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"datasets/uva_course_links.txt\", 'r') as f:\n",
    "        urls = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "    results = []\n",
    "    for url in tqdm(urls, desc=\"Scraping courses\"):\n",
    "        results.append(extract_course_info(url))\n",
    "        \n",
    "    # Convert to DataFrame and save with proper CSV formatting\n",
    "    df = pd.DataFrame(results)\n",
    "        \n",
    "    # Replace None with empty string to avoid issues\n",
    "    df = df.fillna('')\n",
    "        \n",
    "    # Save with quoting all fields and proper encoding\n",
    "    df.to_csv(\n",
    "        \"datasets/recommender_dataset.csv\",\n",
    "        index=False,\n",
    "        quoting=csv.QUOTE_ALL,\n",
    "        quotechar='\"',\n",
    "        encoding='utf-8-sig'  # For proper special character handling\n",
    "    )\n",
    "    print(f\"Successfully saved {len(df)} courses to datasets/recommender_dataset.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559e490-7f49-493b-8ae5-17034a02f9cc",
   "metadata": {},
   "source": [
    "## Deepseek Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6892e-c0d8-4f50-94c0-ef67565432db",
   "metadata": {},
   "source": [
    "NOTE: since I somtimes passed HTML directly to Deepseek, I had to put random apostrophes throughout the HTML code so that it is not automatically executed in the Markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32cd06-b4b2-4a84-bb05-8b6c5ac8f3b6",
   "metadata": {},
   "source": [
    "**Prompt 1**: I have a CSV file with 3802 links to webpages, where each is a webpage for a university course. I want to extract the following from these webpages:\n",
    "1. Course name\n",
    "2. Course catalogue number\n",
    "3. Time period(s)\n",
    "4. College/graduate\n",
    "5. Langauge of instruction\n",
    "6. Is part of\n",
    "7. Course description\n",
    "Here is the html file format that these webpages follow. Use this to write code to extract the previously mentioned characteristics from the webpages inside the csv file:\n",
    "\n",
    "\n",
    "**Prompt 2**: its actually a .txt file that contains the course links, not a csv file, my bad\n",
    "\n",
    "**Prompt 3**: This script works great, except there are two problems:\n",
    "1. The course name is not saved properly. Change the code so that it saves it properly. I will also provide the html for the website once more, the course name is found in: <div''' class=\"inner-spacer\"><h1'> 9A: Machine Learning and Deep Learning Brush-up</h1>\n",
    "2. The time period is not saved properly. The way the website represents the time period is by six boxes, as seen in the html code as <div class=\"block-3 red\"'><img src=\"/xmlpages/resources/TXP/uva/studiegidswebsite/img/red-block.png\" alt=\"\" width=\"16\" height=\"16\"/'>. The way I want you to represent the time period is by looking at the number in the block-x-red section. If it is \"block-3 red\" as shown here, then make the time period \"3\".\n",
    "\n",
    "Now here is the html code for the website:\n",
    "\n",
    "**Prompt 4**: Once agan, the course name is not found properly. Use this part of the html in order to find the course's name:\n",
    "<li' class=\"current\"><a 'href=\"/xmlpages/page/2024-2025-en/search-course/course/122456/\">9A: Machine Learning and Deep Learning Brush-up</a'>.\n",
    "In this example, the course name is 9A: Machine Learning and Deep Learning Brush Up\n",
    "\n",
    "**Prompt 5**: The course names works correctly now. I want you to fix one more thing:\n",
    "The time period is represented by two classes, sem1 and sem2. I want you to represent the semester in the following way:\n",
    "If the red block is in the sem1 class, the number of the red block should correspond to 1-3. If the red block is in the sem2 class, the red block should correspond to 4-6. So if there is block-2 red in sem2, it should be saved as 5. Please modify the code the account for this.\n",
    "\n",
    "**Prompt 6**: The time period might be a range also, so for example the course might be available in semester 1 from block 2-3. Modify the code to account for this range\n",
    "\n",
    "**Prompt 7**: I'd like you to modify the following part of the code, extending the course description to also include objectives, i.e. the h4 heading with id = \"leerdoel\". Combine the objectives and contents in order to make the course description:\n",
    "\n",
    "#' Extract course description (content after the \"Contents\" heading)\n",
    "        contents_heading = soup.find('h4', id='inhoud')\n",
    "        if contents_heading:\n",
    "            description = []\n",
    "            next_elem = contents_heading.find_next_sibling()\n",
    "            while next_elem and next_elem.name != 'h4':\n",
    "                if next_elem.name == 'p':\n",
    "                    description.append(next_elem.get_text(strip=True))\n",
    "                elif next_elem.name == 'ul':\n",
    "                    for li in next_elem.find_all('li'):\n",
    "                        description.append(li.get_text(strip=True))\n",
    "                next_elem = next_elem.find_next_sibling()\n",
    "            course_data['course_description'] = ' '.join(description) if description else None\n",
    "\n",
    "**Prompt 8**: The objectives usually contain bullet points, as can be seen in an example in the following html code:\n",
    "<'ul class=\"bullets-outside\"><l'i>have a better understanding of long-term human impact on the lived environment;</'li><'li>have insight in the ways in which cultures in the past have reacted to environmental change;</'li><'li>understand the effect of modern climate change on archaeological heritage;</li'><'li>have knowledge of the ways in which the discipline of archaeology can contribute to environmental debates;</li'><'li>be aware of theories concerning social and cultural resilience and able to relate these to case studies in different parts of the world.</'li></'ul><'p>\n",
    "\n",
    "**Prompt 9**: This is good, but what I think is happening is that the following code is putting each bullet point etc in a new row of the .csv file, which is not what is meant to happen. Instead, the entire course description should be one string which is inside the course description column:\n",
    "if description_parts:\n",
    "            # Filter out None values and join with newlines\n",
    "            course_data['course_description'] = '\\n'.join(filter(None, [s.strip() if isinstance(s, str) else s for s in description_parts]))\n",
    "        else:\n",
    "            course_data['course_description'] = None\n",
    "\n",
    "**Prompt 10**: what is the escapechar\n",
    "\n",
    "**Prompt 11**: What does quoting =1 do\n",
    "\n",
    "**Prompt 12**: Ok, now we are going to do something tricky: try to also include the semester the course is in. Here is some useful information to take account of when generating the code:\n",
    "1. There are two semesters, semester 1 and semester 2\n",
    "2. Both semesters have 3 blocks inside of them, for a total of 6 blocks. Hence, I'd like the semester to be represented by an integer between 1 and 6, which can also be a range e.g. 2-3\n",
    "3. The semester is decided by red-blocks in the html code. After a red block appears, the time period is also provided. However, you need to account which semester the red block is in. If a red block appears for time period 2-3 in semester 2, the semester in the csv file should be 5-6.\n",
    "4. Here is a snippet of the html code from a course website which include the red blocks, in this case for semester 2 time period 2-3, which would translate to 5-6 if we were to include it in the csv:\n",
    "<'p class=\"meta\">Time period(s)<'/p></'td><'td><'table style=\"width:inherit;\"><'tr class=\"result\"><'th style=\"width:60px\" class=\"sem1\">Sem. 1</'th><'th style=\"width:60px\" class=\"sem2\">Sem. 2</'th></'tr><'tr class=\"result\"><'td class=\"sem1\"><'div class=\"sem-blocks\"><'img src=\"/xmlpages/resources/TXP/uva/studiegidswebsite/img/grey-blocks.png\" alt=\"\" width=\"56\" height=\"16\"/><'div class=\"hide red\"><'img src=\"/xmlpages/resources/TXP/uva/studiegidswebsite/img/red-block.png\" alt=\"\" width=\"16\" height=\"16\"/><'/div></'div><'/td><'td class=\"sem2\"><'div class=\"sem-blocks\"><'img src=\"/xmlpages/resources/TXP/uva/studiegidswebsite/img/grey-blocks.png\" alt=\"\" width=\"56\" height=\"16\"/><'div class=\"block-2-3 red\"><'img src=\"/xmlpages/resources/TXP/uva/studiegidswebsite/img/red-block.png\" alt=\"\" width=\"16\" height=\"16\"/>\n",
    "\n",
    "**Prompt 13**: This did not work. One thing to keep in mind that may help is that there are two classes, sem1 and sem2. Inside each of these classes there is the sem-blocks classes, and within that class there should be the red-blocks, as can be seen in this html code:\n",
    "<'p class=\"meta\">Time period(s)<'/p><'/td><'td><'table style=\"width:inherit;\"><'tr class=\"result\"><'th style=\"width:60px\" class=\"sem1\">Sem. 1</'th><'th style=\"width:60px\" class=\"sem2\">Sem. 2</'th></'tr><'tr class=\"result\"><'td class=\"sem1\"><'div class=\"sem-blocks\"><'img src=\"/xmlpages/resources/TXP/uva/studiegidswebsite/img/grey-blocks.png\" alt=\"\" width=\"56\" height=\"16\"/><'div class=\"hide red\"><'img src=\"/xmlpages/resources/TXP/uva/studiegidswebsite/img/red-block.png\" alt=\"\" width=\"16\" height=\"16\"/></'div></'div></'td><'td class=\"sem2\"><'div class=\"sem-blocks\"><'img src=\"/xmlpages/resources/TXP/uva/studiegidswebsite/img/grey-blocks.png\" alt=\"\" width=\"56\" height=\"16\"/><'div class=\"block-2-3 red\"><'img src=\"/xmlpages/resources/TXP/uva/studiegidswebsite/img/red-block.png\" alt=\"\" width=\"16\" height=\"16\"/>\n",
    "\n",
    "**Prompt 14**: It seems that there is something wrong with the logic somewhere, as the output csv has the time_period column completely empty\n",
    "\n",
    "**Prompt 15**: What does this line do:\n",
    "red_blocks = sem_col.find_all('div', class_=lambda x: x and 'red' in x and 'hide' not in x)\n",
    "\n",
    "**Prompt 16**: This is interesting, as I found using debug statements that tthis line captured the following multiple times:\n",
    "[<'div class=\"hide red\"><' alt=\"\" height=\"16\" src=\"/xmlpages/resources/TXP/uva/studiegidswebsite/img/red-block.png\" width=\"16\"/></'div>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d695a-c53f-4bf7-8f54-d798d535dd30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11774665-b9eb-471f-ba5b-2e92147362c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
